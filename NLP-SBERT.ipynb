{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cd2c44-cb02-46b0-87b7-69762a4bfce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@Author: Magnus Graham\\n1/3/2025\\n\\nThis notebook matches free text to a set of predefined symptoms.\\nIt uses spaCy to preprocess text for, and uses BioBERT and SBERT\\nto map their meaning to the closest possible match in the symptoms list.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@Author: Magnus Graham\n",
    "1/3/2025\n",
    "\n",
    "This notebook matches free text to a set of predefined symptoms.\n",
    "It uses spaCy to preprocess text for, and uses BioBERT and SBERT\n",
    "to map their meaning to the closest possible match in the symptoms list.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3873b194-1d80-4a4c-8952-eea6343d3cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.11/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install torch\n",
    "!pip install spacy\n",
    "!pip install xgboost\n",
    "!python -m spacy download en_core_web_sm  # if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "371f1319-d218-4c76-b1ff-6dceaa6f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "612123eb-14a0-4037-ac4a-552e918c0674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/magnusgraham/NLP'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1de9ea6d-3bbe-42bc-8b6a-043a17ee6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#spaCy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92c58937-0e84-4b08-8924-5f33fc8963e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess input from the user\n",
    "def preprocess_input(user_input):\n",
    "    user_input = user_input.lower().strip()\n",
    "    clauses = [clause.strip() for clause in user_input.split(\",\") if clause.strip()]\n",
    "    return clauses\n",
    "\n",
    "#process clauses of user input\n",
    "def process_clauses(clauses, create_dict=True,lemmatize=True):\n",
    "    print(\"Processing clauses\")\n",
    "    \n",
    "    symptom_correlation = {} if create_dict else None\n",
    "    processed_clauses = [] \n",
    "    \n",
    "    for clause in clauses:\n",
    "        clause_p = clause.replace(\"_\", \" \")\n",
    "        doc = nlp(clause_p)\n",
    "        \n",
    "        processed_clause = \" \".join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    "        \n",
    "        if create_dict:\n",
    "            symptom_correlation[processed_clause] = clause \n",
    "        \n",
    "        processed_clauses.append(processed_clause)\n",
    "    \n",
    "    if create_dict:\n",
    "        return processed_clauses, symptom_correlation\n",
    "    else:\n",
    "        return processed_clauses\n",
    "\n",
    "\n",
    "def process_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    predefined_symptoms = set()\n",
    "    \n",
    "    for col in df.columns[1:19]:  \n",
    "        for value in df[col].unique():\n",
    "            # Add the cleaned symptom to the set\n",
    "            predefined_symptoms.add(str(value))\n",
    "        \n",
    "    return list(predefined_symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e526360-88cc-4b34-8db7-b7b5994cf47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Generate SBERT embeddings for a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of input sentences.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding tensor of shape (batch_size, hidden_size).\n",
    "    \"\"\"\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]  # Ensure input is a list\n",
    "\n",
    "    # Directly encode the sentences\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)  # Output shape: (batch_size, hidden_size)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3168e6c4-7f5d-45ea-bf8a-334e2bf06a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# Function to compute pairwise cosine similarity\n",
    "def cosine_similarity_matrix(embeddings1, embeddings2):\n",
    "    return torch.mm(F.normalize(embeddings1, p=2, dim=1), F.normalize(embeddings2, p=2, dim=1).T)\n",
    "\n",
    "# Function to find the most similar predefined symptoms for each clause\n",
    "def compare_input_to_symptoms(clauses, predefined_symptoms, correlation, output_file, threshold = 0.6, top_n=4):\n",
    "    \n",
    "    print(\"Comparing...\")\n",
    "    clause_embeddings = get_sbert_embeddings(clauses)\n",
    "    symptom_embeddings = get_sbert_embeddings(predefined_symptoms)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity_matrix(clause_embeddings, symptom_embeddings)\n",
    "    \n",
    "    # Process each clause and its similarities to symptoms\n",
    "    \n",
    "    symptom_results = []  # List to hold all symptoms above the threshold\n",
    "    \n",
    "    for i, clause in enumerate(clauses):\n",
    "        # Get similarity scores for the current clause\n",
    "        similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Use a min-heap to maintain top_n elements\n",
    "        top_similar_symptoms = []\n",
    "        for j, similarity in enumerate(similarities):\n",
    "            if similarity >= threshold:\n",
    "                if len(top_similar_symptoms) < top_n:\n",
    "                    heapq.heappush(top_similar_symptoms, (similarity, predefined_symptoms[j]))\n",
    "                else:\n",
    "                    heapq.heappushpop(top_similar_symptoms, (similarity, predefined_symptoms[j]))\n",
    "        \n",
    "        # Extract the top elements from the heap (sorted in descending order by similarity)\n",
    "        top_similar_symptoms.sort(reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        # Display and collect matched symptoms\n",
    "        print(f\"Clause: '{clause}'\")\n",
    "        for similarity, symptom in top_similar_symptoms:\n",
    "            raw_symptom = correlation.get(symptom, symptom)\n",
    "            print(f\"  - Symptom: '{raw_symptom}' - Similarity: {similarity:.2f}\")\n",
    "            symptom_results.append({\"symptom\": raw_symptom})\n",
    "    \n",
    "    # Save the flat list to a JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(symptom_results, f, indent=4)\n",
    "\n",
    "    \n",
    "    print(f\"Output written to {output_file}\")\n",
    "    return symptom_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d1a7e029-32ea-43a3-8ed5-2460373a0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your symptoms separated by commas itching, indigestion, sore stomach, diarrhea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing clauses\n",
      "Processing clauses\n",
      "Comparing...\n",
      "Clause: 'itch'\n",
      "  - Symptom: 'itching' - Similarity: 1.00\n",
      "  - Symptom: ' internal_itching' - Similarity: 0.63\n",
      "Clause: 'indigestion'\n",
      "  - Symptom: ' indigestion' - Similarity: 1.00\n",
      "Clause: 'sore stomach'\n",
      "  - Symptom: ' stomach_pain' - Similarity: 0.85\n",
      "  - Symptom: ' abdominal_pain' - Similarity: 0.76\n",
      "  - Symptom: ' belly_pain' - Similarity: 0.72\n",
      "  - Symptom: ' swelling_of_stomach' - Similarity: 0.70\n",
      "Clause: 'diarrhea'\n",
      "  - Symptom: ' diarrhoea' - Similarity: 0.84\n",
      "  - Symptom: ' vomiting' - Similarity: 0.62\n",
      "  - Symptom: ' nausea' - Similarity: 0.60\n",
      "Output written to output_file.json\n",
      "[\n",
      "    {\n",
      "        \"symptom\": \"itching\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" internal_itching\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" indigestion\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" stomach_pain\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" abdominal_pain\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" belly_pain\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" swelling_of_stomach\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" diarrhoea\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" vomiting\"\n",
      "    },\n",
      "    {\n",
      "        \"symptom\": \" nausea\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load our input and output files\n",
    "data_file = \"DiseaseAndSymptoms.csv\"\n",
    "output_file = \"output_file.json\"\n",
    "\n",
    "#add user inputted symptoms\n",
    "user_input = input('Enter your symptoms separated by commas')\n",
    "\n",
    "# Step 1: Preprocess the input\n",
    "clauses = preprocess_input(user_input)\n",
    "predefined_symptoms = process_csv(data_file)\n",
    "\n",
    "\n",
    "# Step 2: Tokenize and Lemmatize the clauses\n",
    "processed_input = process_clauses(clauses,create_dict=False)\n",
    "processed_symptoms, correlation = process_clauses(predefined_symptoms,create_dict=True)\n",
    "\n",
    "\n",
    "# Step 3: Compare each clause to predefined symptoms using SBERT\n",
    "compare_input_to_symptoms(processed_input, processed_symptoms, correlation, output_file, threshold = 0.6, top_n=4)\n",
    "with open(output_file, \"r\") as f:\n",
    "        file_contents = f.read()\n",
    "        print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494f071-ca9a-423a-b44e-1ab14c54b8d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1397dec-9662-453e-8fb0-fcc2ad2a9310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
